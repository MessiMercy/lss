# SOFAMesh

基于go语言开发。

SOFAMesh 是基于 Istio 改进和扩展而来的 Service Mesh 大规模落地实践方案。在继承 Istio 强大功能和丰富特性的基础上，为满足大规模部署下的性能要求以及应对落地实践中的实际情况，有如下改进：

- 采用 Golang 编写的 MOSN 取代 Envoy
- 合并Mixer到数据平面以解决性能瓶颈
- 增强 Pilot 以实现更灵活的服务发现机制
- 增加对 SOFARPC、Dubbo 的支持



![](../../pic/2020-01-30-15-33-32.png)

Service Mesh 的正式定义：

Service Mesh 是一个 基础设施层，用于处理服务间通讯。现代云原生应用有着复杂的服务拓扑，服务网格负责在这些拓扑中 实现请求的可靠传递。

在实践中，服务网格通常实现为一组 轻量级网络代理，它们与应用程序部署在一起，而 对应用程序透明。

加粗部分是重点：

- 1、基础设施层：这是 Service Mesh 的定位；

- 2、服务间通讯：这是 Service Mesh 的功能和范围；

- 3、实现请求的可靠传递：是 Service Mesh 的目标；

- 4、轻量级网络代理：是 Service Mesh 的部署方式；

- 5、对应用程序透明：是 Service Mesh 的重要特性，零侵入，Service Mesh 的最大优势之一。



![](../../pic/2020-01-30-15-39-20.png)

## 1、sofamesh快速介绍

SOFAMesh 是蚂蚁金服推出的 Service Mesh 开源产品，大家可以简单的理解为是 Istio 的落地增强版本。我们有两个原则：

1、跟随社区

体现在 SOFAMesh 是 fork 自 Istio，而且紧跟 Istio 的最新版本，确保和上游保持同步。我们在 Istio 上的改动都在 SOFAMesh 项目中开源出来，而且在验证完成后我们也会联系 Istio，反哺回上游。

2、实践检验

一切从实践出发，不空谈，在实际生产落地中，发现问题，解决问题。在解决问题的过程中，不将就，不凑合，努力挖掘问题本质，然后追求以技术创新的方式来解决问题。

原则上：Istio 做好的地方，我们简单遵循，保持一致；Istio 做的不好或者疏漏的地方，我们努力改进和弥补。

![](../../pic/2020-01-30-15-41-41.png)

SOFAMesh 的产品规划，这是目前正在进行的第一阶段。架构继续延续 Istio 的数据平面和控制平面分离的方式，主要工作内容是：

- 1、用 Golang 开发 Sidecar，也就是我们的 SOFAMosn 项目，替代 Envoy。

- 2、集成 Istio 和 SOFAMosn，同时针对落地时的需求和问题进行扩展和补充，这是我们的 SOFAMesh 项目

在这个架构中，和 Istio 原版最大的不同在于我们没有选择 Istio 默认集成的 Envoy，而是自己用 Golang 开发了一个名为 SOFAMosn 的 Sidecar 来替代 Envoy。


为什么？

![](../../pic/2020-01-30-15-46-42.png)

MOSN 的全称是 "Modular Observable Smart Network"，正如其名所示，这是一个模块化可观察的智能网络。

Sidecar 模式是 MOSN 目前的主要形式之一，参照 Envoy 项目的定位。我们实现了 Envoy 的 xDS API，和 Istio 保持兼容。

在 Istio 和 Envoy 中，对通讯协议的支持，主要体现在 HTTP/1.1 和 HTTP/2 上，这两个是 Istio / Envoy 中的一等公民。而基于 HTTP/1.1 的 REST 和基于 HTTP/2 的 gRPC，一个是目前社区最主流的通讯协议，一个是未来的主流，Google 的宠儿，CNCF 御用的 RPC 方案，这两个组成了目前 Istio 和 Envoy（乃至 CNCF 所有项目）的黄金组合。

而我们 SOFAMesh，在第一时间就遇到和 Istio/Envoy 不同的情况，我们需要支持 REST 和 gRPC 之外的众多协议：

- SOFARPC：这是蚂蚁金服大量使用的 RPC 协议 (已开源)；

- HSF RPC：这是阿里集团内部大量使用的 RPC 协议 (未开源)；

- Dubbo RPC: 这是社区广泛使用的 RPC 协议 (已开源)；

其他私有协议：在过去几个月间，我们收到需求，期望在 SOFAMesh 上运行其他 TCP 协议，大部分是私有协议。

为此，我们需要考虑在 SOFAMesh 和 SOFAMosn 中增加这些通讯协议的支持，尤其是要可以让我们的客户非常方便的扩展支持各种私有 TCP 协议。


为此，我们需要考虑在 SOFAMesh 和 SOFAMosn 中增加这些通讯协议的支持，尤其是要可以让我们的客户非常方便的扩展支持各种私有 TCP 协议。

## 2、为什么选择golang开发SOFAMosn？


为什么不直接使用 Envoy？

几乎所有了解 SOFAMesh 产品的同学，都会问到这个问题，也是 SOFAMesh 被质疑和非议最多的地方。因为目前 Envoy 的表现的确是性能优越，功能丰富，成熟稳定。

我们在技术选型时也是重点研究过 Envoy，可以说 Envoy 非常符合我们的需求，除了一个地方：Envoy 是 c++。

![](../../pic/2020-01-30-15-57-26.png)


这里有个选择的问题，就是数据平面应该选择什么样的编程语言？

图中列出了目前市场上主要的几个 Service Mesh 类产品在数据平面上的编程语言选择。

- 首先，基于 Java 和 Scala 的第一时间排除，实践证明，JDK/JVM/ 字节码这些方式在部署和运行时都显得太重，不适合作为 Sidecar；

- Nginmesh 的做法有些独特，通过 Golang 的 agent 得到信息然后生成配置文件塞给 nginx，实在不是一个正统的做法；

- Conduit（后更名为 Linkerd2.0）选择的 Rust 是个剑走偏锋的路子，Rust 本身极其适合做数据平面，但是 Rust 语言的普及程度和社区大小是个极大的欠缺，选择 Rust 意味着基本上无法从社区借力；

- Envoy 选择的 c++；

- 国内华为和新浪微博选择了 Golang。

我们在选择之前，内部做过深入讨论，焦点在于：未来的新一代架构的底层平台，编程语言栈应该是什么？最终一致觉得应该是 Golang，配合部分 Java。

对于 Sidecar 这样一个典型场景：

- 要求高性能，低资源消耗，有大量的并发和网络编程；

- 要能被团队快速掌握，尤其新人可以快速上手；

- 要和底层的 k8s 等基础设施频繁交互，未来有 Cloud Native 的大背景；

- 非常重要的：要能被社区和未来的潜在客户接受和快速掌握，不至于在语言层面上有过高的门槛；

不考虑其他因素，满足 Sidecar 场景的最理想的编程语言，自然是非 Golang 莫属。

![](../../pic/2020-01-30-16-00-21.png)

可以说，短期内看，选择 Envoy 远比自行开发 Golang 版本要现实而明智。

但是，前面我们有说到，对于 MOSN 项目，我们有非常宏大的蓝图：准备将原有的网络和中间件方面的各种能力重新沉淀和打磨，打造成为未来新一代架构的底层平台，承载各种服务通讯的职责。这是一个需要一两年时间打造，满足未来三五年乃至十年需求的长期规划项目，我们如果选择以 Envoy 为基础，短期内自然一切 OK，快速获得各种红利，迅速站稳脚跟。

但是：后果是什么？Envoy 是 C++ 的，选择 Envoy 意味着我们后面沉淀和打磨的未来通讯层核心是 c++ 的，我们的语言栈将不得不为此修改为以 c++ 为主，这将严重偏离既定的 Golang + Java 的语言栈规划。



而一旦将时间放到三五年乃至十年八年这个长度时，选择 Envoy 的劣势就出来了：

- C++ 带来的开发和维护成本时远超 Golang，时间越长，改动越多，参与人数越多，使用场景越多，差别越明显；

- 从目前的需求上看，对 Envoy 的扩展会非常多，包括通讯协议和功能。考虑到未来控制平面上可能出现的各种创新，必然需要数据平面做配合，改动会长期存在；

- Golang 还是更适合云原生时代，选择 Golang，除了做 Sidecar，锻炼出来的团队还可以用 Golang 去完成其他各种产品。当然选择 Envoy 也可以这么干，但是这样一来以后系统中就真的都是 c++ 的产品了；

- 另外 Envoy 目前的官方定位只有 Sidecar 一种模式，而我们规划中的 MSON 项目覆盖了各种服务通讯的场景；

- 日后如何和 Envoy 协调是个大难题。尤其我们后续会有非常多的创新想法，也会容许快速试错以鼓励创新，选择  Envoy 在这方面会有很多限制。

所以，最后我们的选择是：先难后易，着眼未来。忍痛（真的很痛）舍弃 Envoy，选择用 Golang 努力打造我们的 SOFAMosn 项目。



对于同样面临要不要选择 Envoy 的同学，我给出的建议是：Envoy 是否适合，取决于是不是想“动”它。

- 如果只是简单的使用，或者少量的扩展，那么其实你接触到的只是 Envoy 在冰山上的这一小部分，这种情况下建议你直接选择 Envoy；

- 如果你和我们一样，将 Service Mesh 作为未来架构的核心，预期会有大量的改动和扩展，同时你又不愿意让自己的主流编程语言技术栈中 c++ 占据主流，那么可以参考我们的选择。

当然，对于原本就是以 c/c++ 为主要编程语言栈的同学来说，不存在这个问题。


## 3、SOFAMesh 在落地期间遇到的典型问题

![](../../pic/2020-01-30-16-06-40.png)

### 1、通信协议扩展

![](../../pic/2020-01-30-16-07-46.png)

### 2、让传统架构的存量应用上 Service Mesh 的问题

![](../../pic/2020-01-30-16-09-11.png)

就是刚才说的现有大量的基于 SOA 框架的程序，这些应用以传统的 SOA 方式开发，如果直接挪到 Service Mesh 下，如 Istio，会遇到问题：因为 Istio 用的服务注册是通过 k8s 来进行，而 k8s 的服务注册模型和原有的 SOA 模型是不匹配的。

SOA 框架当中，通常是以接口为单位来做服务注册，也就是一个应用里面部署多个接口的，在运行时是一个进程里面有多个接口（或者说多个服务）。实际上是以接口为粒度，服务注册和服务发现，包括服务的调用都是以接口为粒度。但是有个问题，部署到 Istio 中后，Istio 做服务注册是以服务为粒度来做服务注册，这个时候不管是注册模型，还是按接口调用的方式都不一致，就是说通过 Interface 调用是调不通的。

左边的代码实例，大家可以看得到，一般情况下 Dubbo 程序是按照 Interface 来注册和发现，调用时也是通过 Interface 来调用。另外，在这个地方，除了通过接口调用之外，还有另外一个问题：服务注册和服务发现的模型，从原来的一对 N，也就是一个进程 N 个接口，变成了要一对一，一个进程一个服务。

怎么解决这个问题？最正统的做法是，是先进行 微服务改造：把原有的 SOA 的架构改成微服务的架构，把现有应用拆分为多个微服务应用，每个应用里面一个服务（或者说接口），这样应用和服务的关系就会变成一对一，服务注册模型就可以匹配。

但是在执行时会有难处，因为微服务改造是一个比较耗时间的过程。我们遇到的实际的需求是：能不能先不做微服务改造，而先上 Service Mesh ？因为 Service Mesh 的功能非常有吸引力，如流量控制，安全加密。那能不能先把应用搬迁到 Service Mesh 上来，先让应用跑起来，后面再慢慢的来做微服务改造。

这就是我们实际遇到的场景，我们需要找到方案来解决问题：注册模型不匹配，原有用接口调用的代码调不通。


![](../../pic/2020-01-30-16-13-00.png)

我们设计了一个名为 DNS 通用选址方案 的解决方案，用来支持 Dubbo 等 SOA 框架，容许通过接口名来调用服务。

细节不太适合展开，给大家介绍最基本的一点，就是说我们会在 DNS 中增加记录，如图上左下角所示标红的三个接口名，我们会在 DNS 中把这个三个接口指向当前服务的 Cluster IP。k8s 的 Cluster IP 通常是一个非常固定的一个 IP，每个服务在 k8s 部署时都会分配。

在增加完 DNS 记录之后，再通过 Interface 的方式去调用，中间在我们的 Service Mesh 里面，我们会基于 Cluster IP 信息完成实际的寻址，并跑通 Istio 的所有功能，和用服务名调用等同。

这个功能在现有的 SOFAMesh 中已经完全实现，大家可以去试用。稍后我们会将这个方案提交给 k8s 或者 Istio 社区，看看他们是否愿意接受这样一个更通用的寻址方式。

在这里我们提出这样一个设想：先上车后补票。所谓"先上车"是指说先上 Service Mesh 的车，"后补票"是指后面再去补微服务拆分的票。好处是在微服务拆分这个巨大的工作量完成之前，提前受益于 Service Mesh 提供的强大功能；同时也可以让部署变得舒服，因为不需要强制先全部完成微服务拆分才能上 Service Mesh 。有了这个方案，就可以在应用不做微服务拆分的情况下运行在 Service Mesh 上，然后再从容的继续进行微服务拆分的工作，这是我们提出这个解决方案的最大初衷。


MOSN 和 x-protocol 介绍：

- Service Mesh 数据平面 SOFAMosn 深层揭秘

http://www.servicemesher.com/blog/sofa-mosn-deep-dive/

- 蚂蚁金服开源 Go 语言版 Service Mesh 数据平面 SOFAMosn 性能报告

http://www.servicemesher.com/blog/sofa-mosn-performance-report-0.1.0/

- 蚂蚁金服开源的 SOFAMesh 的通用协议扩展解析

http://www.servicemesher.com/blog/ant-financial-sofamesh-common-protocol-extension/

- Dubbo on x-protocol——SOFAMesh 中的 x-protocol 示例演示

http://www.servicemesher.com/blog/dubbo-on-x-protocol-in-sofa-mesh/






X-protocol 特性的详细讲解：

- SOFAMesh 中的多协议通用解决方案 x-protocol 介绍系列 (1)-DNS 通用寻址方案

https://skyao.io/post/201809-xprotocol-common-address-solution/

- SOFAMesh 中的多协议通用解决方案 x-protocol 介绍系列 (2)- 快速解码转发

https://skyao.io/post/201809-xprotocol-rapid-decode-forward/

- SOFAMesh 中的多协议通用解决方案 x-protocol 介绍系列 (3)-TCP 协议扩展]

https://skyao.io/post/201809-xprotocol-tcp-protocol-extension/



> 总结一下，我们解决了如下几个问题：

- 1、可以快速的用几个小时就在 SOFAMesh 中添加一个新的通讯协议；

- 2、可以让 SOA 应用在 SOFAMesh 上继续通过接口进行调用，不需要改代码；

- 3、可以实现不做 SOA 程序的微服务改造，就直接搬迁到 SOFAMesh，提前受益。


### 3、涉及到流量劫持的方案

![](../../pic/2020-01-30-16-18-45.png)


Service Mesh 有一个很重要的特性，就是无侵入，而无侵入通常是通过流量劫持来实现的。通过劫持流量，在客户端服务器端无感知的情况下，可以将 Service Mesh 的功能插进去。通常特别适合于类似安全加密等和现有应用的业务逻辑完全分离的场合。



## 4、对服务间通讯范围的探索

![](../../pic/2020-01-30-16-22-09.png)



Service Mesh 即系统内部各个服务之间的通讯，而这通常都是同步的，走 REST 或者 RPC 协议。

在 Service Mesh 的实践过程中，我们发现，Service Mesh 可以提供的功能：

- 请求转发：如服务发现，负载均衡等；

- 路由能力：如强大的 Content Based Routing 和 Version Based Routing ；

- 服务治理：基于路由能力而来的灰度发布，蓝绿部署，版本管理和控制；

- 纠错能力：限流，熔断，重试，测试目的的错误注入；

- 安全类：身份，认证，授权，鉴权，加密等。

可以适用于 Service Mesh 之外的其他领域，也就是说我们可以在其他领域引入并重用这些能力，实现比单纯的东西向通讯更广泛的服务间通讯。



![](../../pic/2020-01-30-16-24-55.png)

第一个探索的方向是 API Gateway，和东西向通讯直接对应的南北向通讯。

主要原因是南北向通讯和东西向通讯在功能上高度重叠，如服务发现，负载均衡，路由，灰度，安全，认证，加密，限流，熔断...... 因此，重用东西向通讯的这些能力就成为自然而然的想法。

传统侵入式框架下，重用这些能力的方式是基于类库方式，也就是在 API Gateway 的实现中，典型如 Zuul，引入东西向通讯中的类库。而 Service Mesh 下，思路有所不同，重用的不再是类库，而是 Sidecar：通过将 Sidecar 用于南北向通讯，重用 Sidecar 的请求转发和服务治理功能。

将 Service Mesh 引入 API Gateway 的优势在于:

- 统一微服务和 API Gateway 两套体系；

- 大量节约学习 / 开发 / 维护的成本；

- 可以在南北向通讯中获得 Service Mesh 的各种特性；

- 可以通过 Service Mesh 的控制平面加强对南北向通讯的控制力。

这个方向上，业界也有一些探索：

- Ambassador: Kubernetes-native microservices API gateway，基于 Envoy 构建，开源项目；

- Gloo: The Function Gateway built on top of Envoy，同样是基于 Envoy，不过这个不仅仅用于传统的微服务 API Gateway，也可以用于 Serverless 架构的 Function；

- Kong：在最近宣布，即将发布的 1.0 版本，kong 将不再是单纯的 API Gateway，而是转型为服务控制平台。可谓是一个反向的探索案例：从 API Gateway 向 Service Mesh 切。

而我们的思路也非常明确：在 SOFAMesh 和 SOFAMosn 的基础上，打造新的 API Gateway 产品，以此来统一东西向通讯和南北向通讯。目前该项目已经启动，后续也会作为开源项目公布出来，对这个话题有兴趣的同学可以保持关注。


![](../../pic/2020-01-30-16-27-17.png)


![](../../pic/2020-01-30-16-27-53.png)


这是我们目前探索和规划中的服务间通讯的完整蓝图：

- Service Mesh

负责东西向通讯，实践中就是我们的 SOFAMesh 产品，基于 Istio 的扩展增强版；

- API Gateway

负责南北向通讯，还在探索中，我们在尝试基于 SOFAMosn 和 SOFAMesh 开发新的 API Gateway 产品；

- Serverless

负责异步通讯，事件驱动模型，粒度也从服务级别细化到 Function 级别，目前在积极探索和实践 knative。

这里给出一个我们的预测：在云原生的时代，服务间通讯的未来都会是  Service Mesh 这种方式，将服务间通讯的职责剥离并下沉。


## 5、基础设置对服务网格的意义

![](../../pic/2020-01-30-16-29-52.png)

里面有一个时代背景：Cloud Native，云原生。而在今年 6 月，CNCF 技术监督委员会通过了 Cloud Native 的定义，中文翻译如上。

这里我们将关注点放在标红的这一句来：云原生的代表技术包括容器、服务网格、微服务、不可变基础设施和声明式 API。


对于云原生架构，蚂蚁金服的策略是：积极拥抱! 我们未来的架构也会往这个方向演进。

对于前面列举的云原生代表技术：

- 容器：大阿里在容器技术上有非常深度的积累，实践多年，而新版本的 Sigma3.* 版本也将基于 k8s；

- 微服务：微服务的前身，SOA 服务化，在大阿里也是实践多年， Dubbo / HSF / SOFA 可谓名满江湖，目前也在陆陆续续的微服务改造中；

- 不可变基础设施和声明式 API：也是高度认可和长期实践的技术。

![](../../pic/2020-01-30-16-31-57.png)

![](../../pic/2020-01-30-16-32-36.png)

Service Mesh 的归宿，或者说最终的形态，是下沉到基础设施！

从 Service Mesh 的发展看，从简单的 Proxy，到功能完善的 Sidecar（如 Linkerd 和 Envoy），再到以 Istio 为代表的第二代 Service Mesh，演进的方式如上图：

第一步：从应用剥离

通过将原有的方法调用改为远程调用，将类库的功能套上 Proxy 的壳子，Service Mesh 成功的将服务间通讯从程序中剥离出来，从此服务间通讯不再是应用程序的一部分。

这一点是大家最容易接受的，对吧？这一步也是最容易实现的，只要搭起来一个 Sidecar 或者说 Proxy，将原有类库的功能塞进去就好了。

第二步：下沉为抽象层

这些剥离出来的服务间通讯的能力，在剥离之后，开始下沉，在应用程序下形成一个单独的抽象层，成为 服务间通讯专用基础设施层。此时，这些能力以一个完成的形态出现，不再存在单独的类库或者框架形式。

第二步和第一步往往是一脉相承的，一旦走出了第一步，自然而然会继续。因为服务间通讯被抽取出来之后，继续往前发展，就会很自然地把它就变成一个基础设施层。

第三步：融入基础设施

继续下沉，和底层基础设施密切联系，进而融为一体，成为平台系统的一部分，典型就是和 kubernetes 结合。

Istio 在这方面做了一个非常大的创新，Istio 的创新，不仅仅在于增加控制平面，也在于和 kubernetes 的结合。

这里抛出一个问题，和传统的 Spring Cloud，Dubbo 等侵入式框架相比：

Service Mesh 的本质差异在哪里？

如果去年的我来回答这个问题，那我会告诉你：下移，沉淀，形成一个通讯层。而今天，我会告诉大家，除了这点之外，还有第二点：充分利用底层基础设施。这是 Dubbo，Spring Cloud 从来没有做到的！

这是今天最想和大家分享的观点，也是过去一年实践中最大的感悟：

Service Mesh 和 Spring Cloud / Dubbo 的本质差异，不仅仅在于将服务间通讯从应用程序中剥离出来，更在于一路下沉到基础设施层并充分利用底层基础设施的能力。

## 6、性能问题

![](../../pic/2020-01-30-16-44-20.png)

为什么大家对性能这么关注？

因为在 Service Mesh 工作原理的各种介绍中，都会提到 Service Mesh 是将原来的一次远程调用，改为走Sidecar（而且像 Istio 是客户端和服务器端两次 Sidecar，如上图所示），这样一次远程调用就会变成三次远程调用，对性能的担忧也就自然而然的产生了：一次远程调用变三次远程调用，性能会下降多少？延迟会增加多少？

> Service Mesh 的基本思路

![](../../pic/2020-01-30-16-45-27.png)

在基于 SDK 的方案中，应用既有业务逻辑，也有各种非业务功能。虽然通过 SDK 实现了代码重用，但是在部署时，这些功能还是混合在一个进程内的。

在 Service Mesh 中，我们将 SDK 客户端的功能从应用中剥离出来，拆解为独立进程，以 Sidecar 的模式部署，让业务进程专注于业务逻辑：

1. 业务进程：专注业务实现，无需感知 Mesh；

2. Sidecar 进程：专注服务间通讯和相关能力，与业务逻辑无关；

我们称之为"关注点分离"：业务开发团队可以专注于业务逻辑，而底层的中间件团队（或者基础设施团队）可以专注于业务逻辑之外的各种通用功能。

通过 Sidecar 拆分为两个独立进程之后，业务应用和 Sidecar 就可以实现“独立维护”：我们可以单独更新/升级业务应用或者 Sidecar。


我们回到前面的蚂蚁金服 Service Mesh 落地后的性能对比数据：从原理上说，Sidecar 拆分之后，原来 SDK 中的各种功能只是拆分到 Sidecar 中。整体上并没有增减，因此理论上说 SDK 和 Sidecar 性能表现是一致的。由于增加了应用和 Sidecar 之间的远程调用，性能不可避免的肯定要受到影响。

首先我们来解释第一个问题：为什么性能损失那么小，和"一次远程调用变三次远程调用"的直觉不符？

![](../../pic/2020-01-30-16-48-37.png)

推导出来的结果就是有3倍的开销，性能自然会有非常大的影响。

但是，真实世界中的应用不是这样：

1. 业务逻辑的占比很高：Sidecar 转发的资源消耗相比之下要低很多，通常是十倍百倍甚至千倍的差异；

2. SDK 也是有消耗的：即使不考虑各种复杂的功能特性，仅仅就报文（尤其是 Body）序列化的编解码开销也是不低的。而且，客户端和服务器端原有的编解码过程是需要处理 Body 的，而在 Sidecar 中，通常都只是读取 Header 而透传 Body，因此在编解码上要快很多。另外应用和 Sidecar 的两次远程通讯，都是走的 Localhost 而不是真实的网络，速度也要快非常多；

因此，在真实世界中，我们假定业务逻辑百倍于 Sidecar 的开销，而 SDK 十倍于 Sidecar 的开销，则：

![](../../pic/2020-01-30-16-50-03.png)


推导出来的结果，性能开销从111增加到113，大约增加2%。这也就解释了为什么我们实际给出的 Service Mesh 的 CPU 和延迟的性能损失都不大的原因。当然，这里我是刻意选择了100和10这两个系数来拼凑出2%这个估算结果，以迎合我们前面给出“均值约增加2%”的数据。这不是准确数值，只是用来模拟。







# 参考

- [sofa-mesh的github地址](https://github.com/sofastack/sofa-mesh)

- [mosn的github地址](https://github.com/mosn/mosn/blob/master/README_ZH.md)

- [sofa-mesh文档](https://www.sofastack.tech/projects/sofa-mesh/overview/)

- [2018 蚂蚁金服 Service Mesh 实践探索 | Qcon 实录](https://mp.weixin.qq.com/s?__biz=MzUzMzU5Mjc1Nw==&mid=2247484395&idx=1&sn=0210fa2fd78828a05ea29e5eff074e20&chksm=faa0ec31cdd76527ad5c123511b1b5e684db1954920c36c794ee5c7391c867979946ed0f3b77&token=729443254&lang=zh_CN#rd)

- [2019 蚂蚁金服 Service Mesh 深度实践 | QCon 实录](https://www.jianshu.com/p/4e57c5224420)


- [敖小剑博客，Service Mesh项目负责人](https://skyao.io/)